/* tslint:disable */
/* eslint-disable */
/**
 * Speechall API
 * The Speechall REST API provides powerful and flexible speech-to-text capabilities. It allows you to transcribe audio files using various underlying STT providers and models, optionally apply custom text replacement rules, and access results in multiple formats. The API includes standard endpoints for transcription and endpoints compatible with the OpenAI API structure. 
 *
 * The version of the OpenAPI document: 0.1.0
 * 
 *
 * NOTE: This class is auto generated by OpenAPI Generator (https://openapi-generator.tech).
 * https://openapi-generator.tech
 * Do not edit the class manually.
 */


import type { Configuration } from './configuration';
import type { AxiosPromise, AxiosInstance, RawAxiosRequestConfig } from 'axios';
import globalAxios from 'axios';
// Some imports not used depending on template conditions
// @ts-ignore
import { DUMMY_BASE_URL, assertParamExists, setApiKeyToObject, setBasicAuthToObject, setBearerAuthToObject, setOAuthToObject, setSearchParams, serializeDataIfNeeded, toPathString, createRequestFunction } from './common';
import type { RequestArgs } from './base';
// @ts-ignore
import { BASE_PATH, COLLECTION_FORMATS, BaseAPI, RequiredError, operationServerMap } from './base';

/**
 * Common configuration options for transcription, applicable to both direct uploads and remote URLs.
 * @export
 * @interface BaseTranscriptionConfiguration
 */
export interface BaseTranscriptionConfiguration {
    /**
     * 
     * @type {TranscriptionModelIdentifier}
     * @memberof BaseTranscriptionConfiguration
     */
    'model': TranscriptionModelIdentifier;
    /**
     * 
     * @type {TranscriptLanguageCode}
     * @memberof BaseTranscriptionConfiguration
     */
    'language'?: TranscriptLanguageCode;
    /**
     * 
     * @type {TranscriptOutputFormat}
     * @memberof BaseTranscriptionConfiguration
     */
    'output_format'?: TranscriptOutputFormat;
    /**
     * The unique identifier (UUID) of a pre-defined replacement ruleset to apply to the final transcription text.
     * @type {string}
     * @memberof BaseTranscriptionConfiguration
     */
    'ruleset_id'?: string;
    /**
     * Whether to add punctuation. Support varies by model (e.g., Deepgram, AssemblyAI). Defaults to `true`.
     * @type {boolean}
     * @memberof BaseTranscriptionConfiguration
     */
    'punctuation'?: boolean;
    /**
     * Level of timestamp detail (`word` or `segment`). Defaults to `segment`.
     * @type {string}
     * @memberof BaseTranscriptionConfiguration
     */
    'timestamp_granularity'?: BaseTranscriptionConfigurationTimestampGranularityEnum;
    /**
     * Enable speaker diarization. Defaults to `false`.
     * @type {boolean}
     * @memberof BaseTranscriptionConfiguration
     */
    'diarization'?: boolean;
    /**
     * Optional text prompt to guide the transcription model. Support varies (e.g., OpenAI).
     * @type {string}
     * @memberof BaseTranscriptionConfiguration
     */
    'initial_prompt'?: string;
    /**
     * Controls output randomness for supported models (e.g., OpenAI). Value between 0 and 1.
     * @type {number}
     * @memberof BaseTranscriptionConfiguration
     */
    'temperature'?: number;
    /**
     * Enable provider-specific smart formatting (e.g., Deepgram). Defaults vary.
     * @type {boolean}
     * @memberof BaseTranscriptionConfiguration
     */
    'smart_format'?: boolean;
    /**
     * Hint for the number of expected speakers for diarization (e.g., RevAI, Deepgram).
     * @type {number}
     * @memberof BaseTranscriptionConfiguration
     */
    'speakers_expected'?: number;
    /**
     * List of custom words/phrases to improve recognition (e.g., Deepgram, AssemblyAI).
     * @type {Array<string>}
     * @memberof BaseTranscriptionConfiguration
     */
    'custom_vocabulary'?: Array<string>;
}

export const BaseTranscriptionConfigurationTimestampGranularityEnum = {
    Word: 'word',
    Segment: 'segment'
} as const;

export type BaseTranscriptionConfigurationTimestampGranularityEnum = typeof BaseTranscriptionConfigurationTimestampGranularityEnum[keyof typeof BaseTranscriptionConfigurationTimestampGranularityEnum];

/**
 * 
 * @export
 * @interface CreateReplacementRuleset201Response
 */
export interface CreateReplacementRuleset201Response {
    /**
     * The unique identifier (UUID) generated for this ruleset. Use this ID in the `ruleset_id` parameter of transcription requests.
     * @type {string}
     * @memberof CreateReplacementRuleset201Response
     */
    'id': string;
}
/**
 * 
 * @export
 * @interface CreateReplacementRulesetRequest
 */
export interface CreateReplacementRulesetRequest {
    /**
     * A user-defined name for this ruleset for easier identification.
     * @type {string}
     * @memberof CreateReplacementRulesetRequest
     */
    'name': string;
    /**
     * An ordered array of replacement rules. Rules are applied in the order they appear in this list. See the `ReplacementRule` schema for different rule types (exact, regex, regex_group).
     * @type {Array<ReplacementRule>}
     * @memberof CreateReplacementRulesetRequest
     */
    'rules': Array<ReplacementRule>;
}
/**
 * Standard structure for error responses. May include additional properties depending on the error type.
 * @export
 * @interface ErrorResponse
 */
export interface ErrorResponse {
    [key: string]: any;

    /**
     * A human-readable message describing the error.
     * @type {string}
     * @memberof ErrorResponse
     */
    'message': string;
}
/**
 * Defines a replacement rule based on finding an exact string match.
 * @export
 * @interface ExactRule
 */
export interface ExactRule {
    /**
     * Discriminator field identifying the rule type as \'exact\'.
     * @type {string}
     * @memberof ExactRule
     */
    'kind': ExactRuleKindEnum;
    /**
     * The exact text string to search for within the transcription.
     * @type {string}
     * @memberof ExactRule
     */
    'search': string;
    /**
     * The text string to replace the found \'search\' text with.
     * @type {string}
     * @memberof ExactRule
     */
    'replacement': string;
    /**
     * If true, the search will match only if the case is identical. If false (default), the search ignores case.
     * @type {boolean}
     * @memberof ExactRule
     */
    'caseSensitive'?: boolean;
}

export const ExactRuleKindEnum = {
    Exact: 'exact'
} as const;

export type ExactRuleKindEnum = typeof ExactRuleKindEnum[keyof typeof ExactRuleKindEnum];

/**
 * The format of the output, in one of these options: `json`, `text`, `srt`, `verbose_json`, or `vtt`. 
 * @export
 * @enum {string}
 */

export const OpenAIAudioResponseFormat = {
    Json: 'json',
    Text: 'text',
    Srt: 'srt',
    VerboseJson: 'verbose_json',
    Vtt: 'vtt'
} as const;

export type OpenAIAudioResponseFormat = typeof OpenAIAudioResponseFormat[keyof typeof OpenAIAudioResponseFormat];


/**
 * Represents a transcription response returned by model, based on the provided input.
 * @export
 * @interface OpenAICreateTranscriptionResponseJson
 */
export interface OpenAICreateTranscriptionResponseJson {
    /**
     * The transcribed text.
     * @type {string}
     * @memberof OpenAICreateTranscriptionResponseJson
     */
    'text': string;
}
/**
 * Represents a verbose json transcription response returned by model, based on the provided input.
 * @export
 * @interface OpenAICreateTranscriptionResponseVerboseJson
 */
export interface OpenAICreateTranscriptionResponseVerboseJson {
    /**
     * The language of the input audio.
     * @type {string}
     * @memberof OpenAICreateTranscriptionResponseVerboseJson
     */
    'language': string;
    /**
     * The duration of the input audio.
     * @type {number}
     * @memberof OpenAICreateTranscriptionResponseVerboseJson
     */
    'duration': number;
    /**
     * The transcribed text.
     * @type {string}
     * @memberof OpenAICreateTranscriptionResponseVerboseJson
     */
    'text': string;
    /**
     * Extracted words and their corresponding timestamps.
     * @type {Array<OpenAITranscriptionWord>}
     * @memberof OpenAICreateTranscriptionResponseVerboseJson
     */
    'words'?: Array<OpenAITranscriptionWord>;
    /**
     * Segments of the transcribed text and their corresponding details.
     * @type {Array<OpenAITranscriptionSegment>}
     * @memberof OpenAICreateTranscriptionResponseVerboseJson
     */
    'segments'?: Array<OpenAITranscriptionSegment>;
}
/**
 * ID of the model to use. It follows the naming convention provider/model-name 
 * @export
 * @interface OpenAICreateTranslationRequestModel
 */
export interface OpenAICreateTranslationRequestModel {
}
/**
 * Standard JSON response for OpenAI-compatible translation requests when `response_format` is `json`. Contains the translated English text.
 * @export
 * @interface OpenAICreateTranslationResponseJson
 */
export interface OpenAICreateTranslationResponseJson {
    /**
     * 
     * @type {string}
     * @memberof OpenAICreateTranslationResponseJson
     */
    'text': string;
}
/**
 * 
 * @export
 * @interface OpenAICreateTranslationResponseVerboseJson
 */
export interface OpenAICreateTranslationResponseVerboseJson {
    /**
     * The language of the output translation (always `english`).
     * @type {string}
     * @memberof OpenAICreateTranslationResponseVerboseJson
     */
    'language': string;
    /**
     * The duration of the input audio.
     * @type {string}
     * @memberof OpenAICreateTranslationResponseVerboseJson
     */
    'duration': string;
    /**
     * The translated text.
     * @type {string}
     * @memberof OpenAICreateTranslationResponseVerboseJson
     */
    'text': string;
    /**
     * Segments of the translated text and their corresponding details.
     * @type {Array<OpenAITranscriptionSegment>}
     * @memberof OpenAICreateTranslationResponseVerboseJson
     */
    'segments'?: Array<OpenAITranscriptionSegment>;
}
/**
 * Represents a segment of transcribed or translated text, based on OpenAI\'s verbose JSON structure.
 * @export
 * @interface OpenAITranscriptionSegment
 */
export interface OpenAITranscriptionSegment {
    /**
     * Unique identifier of the segment.
     * @type {number}
     * @memberof OpenAITranscriptionSegment
     */
    'id': number;
    /**
     * Seek offset of the segment.
     * @type {number}
     * @memberof OpenAITranscriptionSegment
     */
    'seek': number;
    /**
     * Start time of the segment in seconds.
     * @type {number}
     * @memberof OpenAITranscriptionSegment
     */
    'start': number;
    /**
     * End time of the segment in seconds.
     * @type {number}
     * @memberof OpenAITranscriptionSegment
     */
    'end': number;
    /**
     * Text content of the segment.
     * @type {string}
     * @memberof OpenAITranscriptionSegment
     */
    'text': string;
    /**
     * Array of token IDs for the text content.
     * @type {Array<number>}
     * @memberof OpenAITranscriptionSegment
     */
    'tokens': Array<number>;
    /**
     * Temperature parameter used for generating the segment.
     * @type {number}
     * @memberof OpenAITranscriptionSegment
     */
    'temperature': number;
    /**
     * Average logprob of the segment. If the value is lower than -1, consider the logprobs failed.
     * @type {number}
     * @memberof OpenAITranscriptionSegment
     */
    'avg_logprob': number;
    /**
     * Compression ratio of the segment. If the value is greater than 2.4, consider the compression failed.
     * @type {number}
     * @memberof OpenAITranscriptionSegment
     */
    'compression_ratio': number;
    /**
     * Probability of no speech in the segment. If the value is higher than 1.0 and the `avg_logprob` is below -1, consider this segment silent.
     * @type {number}
     * @memberof OpenAITranscriptionSegment
     */
    'no_speech_prob': number;
}
/**
 * Represents a single word identified during transcription, including its start and end times. Included in `verbose_json` response when `word` granularity is requested.
 * @export
 * @interface OpenAITranscriptionWord
 */
export interface OpenAITranscriptionWord {
    /**
     * The text content of the word.
     * @type {string}
     * @memberof OpenAITranscriptionWord
     */
    'word': string;
    /**
     * Start time of the word in seconds.
     * @type {number}
     * @memberof OpenAITranscriptionWord
     */
    'start': number;
    /**
     * End time of the word in seconds.
     * @type {number}
     * @memberof OpenAITranscriptionWord
     */
    'end': number;
}
/**
 * @type OpenaiCompatibleCreateTranscription200Response
 * @export
 */
export type OpenaiCompatibleCreateTranscription200Response = OpenAICreateTranscriptionResponseJson | OpenAICreateTranscriptionResponseVerboseJson;

/**
 * @type OpenaiCompatibleCreateTranslation200Response
 * @export
 */
export type OpenaiCompatibleCreateTranslation200Response = OpenAICreateTranslationResponseJson | OpenAICreateTranslationResponseVerboseJson;

/**
 * Defines a replacement rule that uses regex capture groups to apply different replacements to different parts of the matched text.
 * @export
 * @interface RegexGroupRule
 */
export interface RegexGroupRule {
    /**
     * Discriminator field identifying the rule type as \'regex_group\'.
     * @type {string}
     * @memberof RegexGroupRule
     */
    'kind': RegexGroupRuleKindEnum;
    /**
     * The regular expression pattern containing capture groups `(...)`. The entire pattern must match for replacements to occur.
     * @type {string}
     * @memberof RegexGroupRule
     */
    'pattern': string;
    /**
     * An object where keys are capture group numbers (as strings, e.g., \"1\", \"2\") and values are the respective replacement strings for those groups. Groups not listed are kept as matched. The entire match is reconstructed using these replacements.
     * @type {{ [key: string]: string; }}
     * @memberof RegexGroupRule
     */
    'groupReplacements': { [key: string]: string; };
    /**
     * An array of flags to modify the regex behavior.
     * @type {Array<string>}
     * @memberof RegexGroupRule
     */
    'flags'?: Array<RegexGroupRuleFlagsEnum>;
}

export const RegexGroupRuleKindEnum = {
    RegexGroup: 'regex_group'
} as const;

export type RegexGroupRuleKindEnum = typeof RegexGroupRuleKindEnum[keyof typeof RegexGroupRuleKindEnum];
export const RegexGroupRuleFlagsEnum = {
    I: 'i',
    M: 'm',
    S: 's',
    X: 'x',
    U: 'u'
} as const;

export type RegexGroupRuleFlagsEnum = typeof RegexGroupRuleFlagsEnum[keyof typeof RegexGroupRuleFlagsEnum];

/**
 * Defines a replacement rule based on matching a regular expression pattern.
 * @export
 * @interface RegexRule
 */
export interface RegexRule {
    /**
     * Discriminator field identifying the rule type as \'regex\'.
     * @type {string}
     * @memberof RegexRule
     */
    'kind': RegexRuleKindEnum;
    /**
     * The regular expression pattern to search for. Uses standard regex syntax (implementation specific, often PCRE-like). Remember to escape special characters if needed (e.g., `\\\\.` for a literal dot).
     * @type {string}
     * @memberof RegexRule
     */
    'pattern': string;
    /**
     * The replacement text. Can include backreferences to capture groups from the pattern, like `$1`, `$2`, etc. A literal `$` should be escaped (e.g., `$$`).
     * @type {string}
     * @memberof RegexRule
     */
    'replacement': string;
    /**
     * An array of flags to modify the regex behavior (e.g., \'i\' for case-insensitivity).
     * @type {Array<string>}
     * @memberof RegexRule
     */
    'flags'?: Array<RegexRuleFlagsEnum>;
}

export const RegexRuleKindEnum = {
    Regex: 'regex'
} as const;

export type RegexRuleKindEnum = typeof RegexRuleKindEnum[keyof typeof RegexRuleKindEnum];
export const RegexRuleFlagsEnum = {
    I: 'i',
    M: 'm',
    S: 's',
    X: 'x',
    U: 'u'
} as const;

export type RegexRuleFlagsEnum = typeof RegexRuleFlagsEnum[keyof typeof RegexRuleFlagsEnum];

/**
 * Configuration options for transcribing audio specified by a remote URL via the `/transcribe-remote` endpoint.
 * @export
 * @interface RemoteTranscriptionConfiguration
 */
export interface RemoteTranscriptionConfiguration {
    /**
     * 
     * @type {TranscriptionModelIdentifier}
     * @memberof RemoteTranscriptionConfiguration
     */
    'model': TranscriptionModelIdentifier;
    /**
     * 
     * @type {TranscriptLanguageCode}
     * @memberof RemoteTranscriptionConfiguration
     */
    'language'?: TranscriptLanguageCode;
    /**
     * 
     * @type {TranscriptOutputFormat}
     * @memberof RemoteTranscriptionConfiguration
     */
    'output_format'?: TranscriptOutputFormat;
    /**
     * The unique identifier (UUID) of a pre-defined replacement ruleset to apply to the final transcription text.
     * @type {string}
     * @memberof RemoteTranscriptionConfiguration
     */
    'ruleset_id'?: string;
    /**
     * Whether to add punctuation. Support varies by model (e.g., Deepgram, AssemblyAI). Defaults to `true`.
     * @type {boolean}
     * @memberof RemoteTranscriptionConfiguration
     */
    'punctuation'?: boolean;
    /**
     * Level of timestamp detail (`word` or `segment`). Defaults to `segment`.
     * @type {string}
     * @memberof RemoteTranscriptionConfiguration
     */
    'timestamp_granularity'?: RemoteTranscriptionConfigurationTimestampGranularityEnum;
    /**
     * Enable speaker diarization. Defaults to `false`.
     * @type {boolean}
     * @memberof RemoteTranscriptionConfiguration
     */
    'diarization'?: boolean;
    /**
     * Optional text prompt to guide the transcription model. Support varies (e.g., OpenAI).
     * @type {string}
     * @memberof RemoteTranscriptionConfiguration
     */
    'initial_prompt'?: string;
    /**
     * Controls output randomness for supported models (e.g., OpenAI). Value between 0 and 1.
     * @type {number}
     * @memberof RemoteTranscriptionConfiguration
     */
    'temperature'?: number;
    /**
     * Enable provider-specific smart formatting (e.g., Deepgram). Defaults vary.
     * @type {boolean}
     * @memberof RemoteTranscriptionConfiguration
     */
    'smart_format'?: boolean;
    /**
     * Hint for the number of expected speakers for diarization (e.g., RevAI, Deepgram).
     * @type {number}
     * @memberof RemoteTranscriptionConfiguration
     */
    'speakers_expected'?: number;
    /**
     * List of custom words/phrases to improve recognition (e.g., Deepgram, AssemblyAI).
     * @type {Array<string>}
     * @memberof RemoteTranscriptionConfiguration
     */
    'custom_vocabulary'?: Array<string>;
    /**
     * The publicly accessible URL of the audio file to transcribe. The API server must be able to fetch the audio from this URL.
     * @type {string}
     * @memberof RemoteTranscriptionConfiguration
     */
    'file_url': string;
    /**
     * An array of replacement rules to be applied directly to this transcription request, in order. This allows defining rules inline instead of (or in addition to) using a pre-saved `ruleset_id`.
     * @type {Array<ReplacementRule>}
     * @memberof RemoteTranscriptionConfiguration
     */
    'replacement_ruleset'?: Array<ReplacementRule>;
}

export const RemoteTranscriptionConfigurationTimestampGranularityEnum = {
    Word: 'word',
    Segment: 'segment'
} as const;

export type RemoteTranscriptionConfigurationTimestampGranularityEnum = typeof RemoteTranscriptionConfigurationTimestampGranularityEnum[keyof typeof RemoteTranscriptionConfigurationTimestampGranularityEnum];

/**
 * @type ReplacementRule
 * Defines a single rule for finding and replacing text in a transcription. Use one of the specific rule types (`ExactRule`, `RegexRule`, `RegexGroupRule`). The `kind` property acts as a discriminator.
 * @export
 */
export type ReplacementRule = { kind: 'exact' } & ExactRule | { kind: 'regex' } & RegexRule | { kind: 'regex_group' } & RegexGroupRule;

/**
 * Describes an available speech-to-text model, its provider, capabilities, and characteristics.
 * @export
 * @interface SpeechToTextModel
 */
export interface SpeechToTextModel {
    /**
     * 
     * @type {TranscriptionModelIdentifier}
     * @memberof SpeechToTextModel
     */
    'id': TranscriptionModelIdentifier;
    /**
     * A user-friendly name for the model.
     * @type {string}
     * @memberof SpeechToTextModel
     */
    'display_name': string;
    /**
     * 
     * @type {TranscriptionProvider}
     * @memberof SpeechToTextModel
     */
    'provider': TranscriptionProvider;
    /**
     * A brief description of the model, its intended use case, or version notes.
     * @type {string}
     * @memberof SpeechToTextModel
     */
    'description'?: string | null;
    /**
     * The cost per second of audio processed in USD.
     * @type {number}
     * @memberof SpeechToTextModel
     */
    'cost_per_second_usd'?: number | null;
    /**
     * Indicates whether the model is currently available for use.
     * @type {boolean}
     * @memberof SpeechToTextModel
     */
    'is_available': boolean;
    /**
     * A list of language codes (preferably BCP 47, e.g., \"en-US\", \"en-GB\", \"es-ES\") supported by this model. May include `auto` if automatic language detection is supported across multiple languages within a single audio file. 
     * @type {Array<string>}
     * @memberof SpeechToTextModel
     */
    'supported_languages'?: Array<string> | null;
    /**
     * Indicates whether the model generally supports automatic punctuation insertion.
     * @type {boolean}
     * @memberof SpeechToTextModel
     */
    'punctuation'?: boolean | null;
    /**
     * Indicates whether the model generally supports speaker diarization (identifying different speakers).
     * @type {boolean}
     * @memberof SpeechToTextModel
     */
    'diarization'?: boolean | null;
    /**
     * Indicates whether the model can be used for real-time streaming transcription via a WebSocket connection (if offered by Speechall).
     * @type {boolean}
     * @memberof SpeechToTextModel
     */
    'streamable'?: boolean | null;
    /**
     * An approximate measure of processing speed for batch processing. Defined as (audio duration) / (processing time). A higher value means faster processing (e.g., RTF=2 means it processes 1 second of audio in 0.5 seconds). May not be available for all models or streaming scenarios. 
     * @type {number}
     * @memberof SpeechToTextModel
     */
    'real_time_factor'?: number | null;
    /**
     * The maximum duration of a single audio file (in seconds) that the model can reliably process in one request. May vary by provider or plan.
     * @type {number}
     * @memberof SpeechToTextModel
     */
    'max_duration_seconds'?: number | null;
    /**
     * The maximum size of a single audio file (in bytes) that can be uploaded for processing by this model. May vary by provider or plan.
     * @type {number}
     * @memberof SpeechToTextModel
     */
    'max_file_size_bytes'?: number | null;
    /**
     * The specific version identifier for the model.
     * @type {string}
     * @memberof SpeechToTextModel
     */
    'version'?: string | null;
    /**
     * The date when this specific version of the model was released or last updated.
     * @type {string}
     * @memberof SpeechToTextModel
     */
    'release_date'?: string | null;
    /**
     * The primary type or training domain of the model. Helps identify suitability for different audio types.
     * @type {string}
     * @memberof SpeechToTextModel
     */
    'model_type'?: SpeechToTextModelModelTypeEnum | null;
    /**
     * A general indication of the model\'s expected accuracy level relative to other models. Not a guaranteed metric.
     * @type {string}
     * @memberof SpeechToTextModel
     */
    'accuracy_tier'?: SpeechToTextModelAccuracyTierEnum | null;
    /**
     * A list of audio encodings that this model supports or is optimized for (e.g., LINEAR16, FLAC, MP3, Opus).
     * @type {Array<string>}
     * @memberof SpeechToTextModel
     */
    'supported_audio_encodings'?: Array<string> | null;
    /**
     * A list of audio sample rates (in Hz) that this model supports or is optimized for.
     * @type {Array<number>}
     * @memberof SpeechToTextModel
     */
    'supported_sample_rates'?: Array<number> | null;
    /**
     * Indicates whether the model can provide speaker labels for the transcription.
     * @type {boolean}
     * @memberof SpeechToTextModel
     */
    'speaker_labels'?: boolean | null;
    /**
     * Indicates whether the model can provide timestamps for individual words.
     * @type {boolean}
     * @memberof SpeechToTextModel
     */
    'word_timestamps'?: boolean | null;
    /**
     * Indicates whether the model provides confidence scores for the transcription or individual words.
     * @type {boolean}
     * @memberof SpeechToTextModel
     */
    'confidence_scores'?: boolean | null;
    /**
     * Indicates whether the model supports automatic language detection for input audio.
     * @type {boolean}
     * @memberof SpeechToTextModel
     */
    'language_detection'?: boolean | null;
    /**
     * Indicates if the model can leverage a custom vocabulary or language model adaptation.
     * @type {boolean}
     * @memberof SpeechToTextModel
     */
    'custom_vocabulary_support'?: boolean | null;
    /**
     * Indicates if the model supports filtering or masking of profanity.
     * @type {boolean}
     * @memberof SpeechToTextModel
     */
    'profanity_filtering'?: boolean | null;
    /**
     * Indicates if the model supports noise reduction.
     * @type {boolean}
     * @memberof SpeechToTextModel
     */
    'noise_reduction'?: boolean | null;
    /**
     * Indicates whether the model supports SRT subtitle format output.
     * @type {boolean}
     * @memberof SpeechToTextModel
     */
    'supports_srt': boolean;
    /**
     * Indicates whether the model supports VTT subtitle format output.
     * @type {boolean}
     * @memberof SpeechToTextModel
     */
    'supports_vtt': boolean;
    /**
     * Indicates whether the model supports voice activity detection (VAD) to identify speech segments.
     * @type {boolean}
     * @memberof SpeechToTextModel
     */
    'voice_activity_detection'?: boolean | null;
}

export const SpeechToTextModelModelTypeEnum = {
    General: 'general',
    PhoneCall: 'phone_call',
    Video: 'video',
    CommandAndSearch: 'command_and_search',
    Medical: 'medical',
    Legal: 'legal',
    Voicemail: 'voicemail',
    Meeting: 'meeting'
} as const;

export type SpeechToTextModelModelTypeEnum = typeof SpeechToTextModelModelTypeEnum[keyof typeof SpeechToTextModelModelTypeEnum];
export const SpeechToTextModelAccuracyTierEnum = {
    Basic: 'basic',
    Standard: 'standard',
    Enhanced: 'enhanced',
    Premium: 'premium'
} as const;

export type SpeechToTextModelAccuracyTierEnum = typeof SpeechToTextModelAccuracyTierEnum[keyof typeof SpeechToTextModelAccuracyTierEnum];

/**
 * The language code of the audio file, typically in ISO 639-1 format. Specifying the correct language improves transcription accuracy and speed. The special value `auto` can be used to request automatic language detection, if supported by the selected model. If omitted, the default language is English (`en`). 
 * @export
 * @enum {string}
 */

export const TranscriptLanguageCode = {
    Auto: 'auto',
    En: 'en',
    EnAu: 'en_au',
    EnUk: 'en_uk',
    EnUs: 'en_us',
    Af: 'af',
    Am: 'am',
    Ar: 'ar',
    As: 'as',
    Az: 'az',
    Ba: 'ba',
    Be: 'be',
    Bg: 'bg',
    Bn: 'bn',
    Bo: 'bo',
    Br: 'br',
    Bs: 'bs',
    Ca: 'ca',
    Cs: 'cs',
    Cy: 'cy',
    Da: 'da',
    De: 'de',
    El: 'el',
    Es: 'es',
    Et: 'et',
    Eu: 'eu',
    Fa: 'fa',
    Fi: 'fi',
    Fo: 'fo',
    Fr: 'fr',
    Gl: 'gl',
    Gu: 'gu',
    Ha: 'ha',
    Haw: 'haw',
    He: 'he',
    Hi: 'hi',
    Hr: 'hr',
    Ht: 'ht',
    Hu: 'hu',
    Hy: 'hy',
    Id: 'id',
    Is: 'is',
    It: 'it',
    Ja: 'ja',
    Jw: 'jw',
    Ka: 'ka',
    Kk: 'kk',
    Km: 'km',
    Kn: 'kn',
    Ko: 'ko',
    La: 'la',
    Lb: 'lb',
    Ln: 'ln',
    Lo: 'lo',
    Lt: 'lt',
    Lv: 'lv',
    Mg: 'mg',
    Mi: 'mi',
    Mk: 'mk',
    Ml: 'ml',
    Mn: 'mn',
    Mr: 'mr',
    Ms: 'ms',
    Mt: 'mt',
    My: 'my',
    Ne: 'ne',
    Nl: 'nl',
    Nn: 'nn',
    False: 'false',
    Oc: 'oc',
    Pa: 'pa',
    Pl: 'pl',
    Ps: 'ps',
    Pt: 'pt',
    Ro: 'ro',
    Ru: 'ru',
    Sa: 'sa',
    Sd: 'sd',
    Si: 'si',
    Sk: 'sk',
    Sl: 'sl',
    Sn: 'sn',
    So: 'so',
    Sq: 'sq',
    Sr: 'sr',
    Su: 'su',
    Sv: 'sv',
    Sw: 'sw',
    Ta: 'ta',
    Te: 'te',
    Tg: 'tg',
    Th: 'th',
    Tk: 'tk',
    Tl: 'tl',
    Tr: 'tr',
    Tt: 'tt',
    Uk: 'uk',
    Ur: 'ur',
    Uz: 'uz',
    Vi: 'vi',
    Yi: 'yi',
    Yo: 'yo',
    Zh: 'zh'
} as const;

export type TranscriptLanguageCode = typeof TranscriptLanguageCode[keyof typeof TranscriptLanguageCode];


/**
 * Specifies the desired format of the transcription output. - `text`: Plain text containing the full transcription. - `json_text`: A simple JSON object containing the transcription ID and the full text (`TranscriptionOnlyText` schema). - `json`: A detailed JSON object including segments, timestamps (based on `timestamp_granularity`), language, and potentially speaker labels and provider metadata (`TranscriptionDetailed` schema). - `srt`: SubRip subtitle format (returned as plain text). - `vtt`: WebVTT subtitle format (returned as plain text). 
 * @export
 * @enum {string}
 */

export const TranscriptOutputFormat = {
    Text: 'text',
    JsonText: 'json_text',
    Json: 'json',
    Srt: 'srt',
    Vtt: 'vtt'
} as const;

export type TranscriptOutputFormat = typeof TranscriptOutputFormat[keyof typeof TranscriptOutputFormat];


/**
 * A detailed JSON response format containing the full text, detected language, duration, individual timed segments, and potentially speaker labels and provider-specific metadata. Returned when `output_format` is `json`.
 * @export
 * @interface TranscriptionDetailed
 */
export interface TranscriptionDetailed {
    /**
     * A unique identifier for the transcription job/request.
     * @type {string}
     * @memberof TranscriptionDetailed
     */
    'id': string;
    /**
     * The full transcribed text as a single string.
     * @type {string}
     * @memberof TranscriptionDetailed
     */
    'text': string;
    /**
     * The detected or specified language of the audio (ISO 639-1 code).
     * @type {string}
     * @memberof TranscriptionDetailed
     */
    'language'?: string;
    /**
     * The total duration of the processed audio file in seconds. **Deprecated**: This property may be removed in future versions as duration analysis might occur asynchronously. Rely on segment end times for duration information if needed. 
     * @type {number}
     * @memberof TranscriptionDetailed
     * @deprecated
     */
    'duration'?: number;
    /**
     * An array of transcribed segments, providing time-coded chunks of the transcription. The level of detail (word vs. segment timestamps) depends on the `timestamp_granularity` request parameter. May include speaker labels if diarization was enabled.
     * @type {Array<TranscriptionSegment>}
     * @memberof TranscriptionDetailed
     */
    'segments'?: Array<TranscriptionSegment>;
    /**
     * An array of transcribed words, providing time-coded chunks of the transcription. The level of detail (word vs. segment timestamps) depends on the `timestamp_granularity` request parameter. May include speaker labels if diarization was enabled.
     * @type {Array<TranscriptionWord>}
     * @memberof TranscriptionDetailed
     */
    'words'?: Array<TranscriptionWord>;
    /**
     * An optional object containing additional metadata returned directly from the underlying STT provider. The structure of this object is provider-dependent.
     * @type {{ [key: string]: any; }}
     * @memberof TranscriptionDetailed
     * @deprecated
     */
    'provider_metadata'?: { [key: string]: any; };
}
/**
 * Unique identifier for a specific Speech-to-Text model, composed as `provider.model_name`. Used to select the engine for transcription.
 * @export
 * @enum {string}
 */

export const TranscriptionModelIdentifier = {
    AmazonTranscribe: 'amazon.transcribe',
    AssemblyaiBest: 'assemblyai.best',
    AssemblyaiNano: 'assemblyai.nano',
    AssemblyaiSlam1: 'assemblyai.slam-1',
    AssemblyaiUniversal: 'assemblyai.universal',
    AzureStandard: 'azure.standard',
    CloudflareWhisper: 'cloudflare.whisper',
    CloudflareWhisperLargeV3Turbo: 'cloudflare.whisper-large-v3-turbo',
    CloudflareWhisperTinyEn: 'cloudflare.whisper-tiny-en',
    DeepgramBase: 'deepgram.base',
    DeepgramBaseConversationalai: 'deepgram.base-conversationalai',
    DeepgramBaseFinance: 'deepgram.base-finance',
    DeepgramBaseGeneral: 'deepgram.base-general',
    DeepgramBaseMeeting: 'deepgram.base-meeting',
    DeepgramBasePhonecall: 'deepgram.base-phonecall',
    DeepgramBaseVideo: 'deepgram.base-video',
    DeepgramBaseVoicemail: 'deepgram.base-voicemail',
    DeepgramEnhanced: 'deepgram.enhanced',
    DeepgramEnhancedFinance: 'deepgram.enhanced-finance',
    DeepgramEnhancedGeneral: 'deepgram.enhanced-general',
    DeepgramEnhancedMeeting: 'deepgram.enhanced-meeting',
    DeepgramEnhancedPhonecall: 'deepgram.enhanced-phonecall',
    DeepgramNova: 'deepgram.nova',
    DeepgramNovaGeneral: 'deepgram.nova-general',
    DeepgramNovaPhonecall: 'deepgram.nova-phonecall',
    DeepgramNova2: 'deepgram.nova-2',
    DeepgramNova2Atc: 'deepgram.nova-2-atc',
    DeepgramNova2Automotive: 'deepgram.nova-2-automotive',
    DeepgramNova2Conversationalai: 'deepgram.nova-2-conversationalai',
    DeepgramNova2Drivethru: 'deepgram.nova-2-drivethru',
    DeepgramNova2Finance: 'deepgram.nova-2-finance',
    DeepgramNova2General: 'deepgram.nova-2-general',
    DeepgramNova2Medical: 'deepgram.nova-2-medical',
    DeepgramNova2Meeting: 'deepgram.nova-2-meeting',
    DeepgramNova2Phonecall: 'deepgram.nova-2-phonecall',
    DeepgramNova2Video: 'deepgram.nova-2-video',
    DeepgramNova2Voicemail: 'deepgram.nova-2-voicemail',
    DeepgramNova3: 'deepgram.nova-3',
    DeepgramNova3General: 'deepgram.nova-3-general',
    DeepgramNova3Medical: 'deepgram.nova-3-medical',
    DeepgramWhisper: 'deepgram.whisper',
    DeepgramWhisperBase: 'deepgram.whisper-base',
    DeepgramWhisperLarge: 'deepgram.whisper-large',
    DeepgramWhisperMedium: 'deepgram.whisper-medium',
    DeepgramWhisperSmall: 'deepgram.whisper-small',
    DeepgramWhisperTiny: 'deepgram.whisper-tiny',
    FalaiElevenlabsSpeechToText: 'falai.elevenlabs-speech-to-text',
    FalaiSpeechToText: 'falai.speech-to-text',
    FalaiWhisper: 'falai.whisper',
    FalaiWizper: 'falai.wizper',
    FireworksaiWhisperV3: 'fireworksai.whisper-v3',
    FireworksaiWhisperV3Turbo: 'fireworksai.whisper-v3-turbo',
    GladiaStandard: 'gladia.standard',
    GoogleEnhanced: 'google.enhanced',
    GoogleStandard: 'google.standard',
    GeminiGemini25FlashPreview0520: 'gemini.gemini-2.5-flash-preview-05-20',
    GeminiGemini25ProPreview0605: 'gemini.gemini-2.5-pro-preview-06-05',
    GeminiGemini20Flash: 'gemini.gemini-2.0-flash',
    GeminiGemini20FlashLite: 'gemini.gemini-2.0-flash-lite',
    GroqDistilWhisperLargeV3En: 'groq.distil-whisper-large-v3-en',
    GroqWhisperLargeV3: 'groq.whisper-large-v3',
    GroqWhisperLargeV3Turbo: 'groq.whisper-large-v3-turbo',
    IbmStandard: 'ibm.standard',
    OpenaiWhisper1: 'openai.whisper-1',
    OpenaiGpt4oTranscribe: 'openai.gpt-4o-transcribe',
    OpenaiGpt4oMiniTranscribe: 'openai.gpt-4o-mini-transcribe',
    RevaiMachine: 'revai.machine',
    RevaiFusion: 'revai.fusion',
    SpeechmaticsEnhanced: 'speechmatics.enhanced',
    SpeechmaticsStandard: 'speechmatics.standard'
} as const;

export type TranscriptionModelIdentifier = typeof TranscriptionModelIdentifier[keyof typeof TranscriptionModelIdentifier];


/**
 * A simplified JSON response format containing only the transcription ID and the full transcribed text. Returned when `output_format` is `json_text`.
 * @export
 * @interface TranscriptionOnlyText
 */
export interface TranscriptionOnlyText {
    /**
     * A unique identifier for the transcription job/request.
     * @type {string}
     * @memberof TranscriptionOnlyText
     */
    'id': string;
    /**
     * The full transcribed text as a single string.
     * @type {string}
     * @memberof TranscriptionOnlyText
     */
    'text': string;
}
/**
 * The identifier for the underlying Speech-to-Text service provider (e.g., \'openai\', \'deepgram\').
 * @export
 * @enum {string}
 */

export const TranscriptionProvider = {
    Amazon: 'amazon',
    Assemblyai: 'assemblyai',
    Azure: 'azure',
    Cloudflare: 'cloudflare',
    Deepgram: 'deepgram',
    Falai: 'falai',
    Fireworksai: 'fireworksai',
    Gemini: 'gemini',
    Gladia: 'gladia',
    Google: 'google',
    Groq: 'groq',
    Ibm: 'ibm',
    Openai: 'openai',
    Revai: 'revai',
    Speechmatics: 'speechmatics'
} as const;

export type TranscriptionProvider = typeof TranscriptionProvider[keyof typeof TranscriptionProvider];


/**
 * @type TranscriptionResponse
 * Represents the JSON structure returned when a JSON-based `output_format` (`json` or `json_text`) is requested. It can be either a detailed structure or a simple text-only structure.
 * @export
 */
export type TranscriptionResponse = TranscriptionDetailed | TranscriptionOnlyText;

/**
 * Represents a time-coded segment of the transcription, typically corresponding to a phrase, sentence, or speaker turn.
 * @export
 * @interface TranscriptionSegment
 */
export interface TranscriptionSegment {
    /**
     * The start time of the segment in seconds from the beginning of the audio.
     * @type {number}
     * @memberof TranscriptionSegment
     */
    'start'?: number;
    /**
     * The end time of the segment in seconds from the beginning of the audio.
     * @type {number}
     * @memberof TranscriptionSegment
     */
    'end'?: number;
    /**
     * The transcribed text content of this segment.
     * @type {string}
     * @memberof TranscriptionSegment
     */
    'text'?: string;
    /**
     * An identifier for the speaker of this segment, present if diarization was enabled and successful.
     * @type {string}
     * @memberof TranscriptionSegment
     */
    'speaker'?: string;
    /**
     * The model\'s confidence score for the transcription of this segment, typically between 0 and 1 (if provided by the model).
     * @type {number}
     * @memberof TranscriptionSegment
     */
    'confidence'?: number;
}
/**
 * Represents a word in the transcription, providing time-coded chunks of the transcription.
 * @export
 * @interface TranscriptionWord
 */
export interface TranscriptionWord {
    /**
     * The start time of the word in seconds from the beginning of the audio.
     * @type {number}
     * @memberof TranscriptionWord
     */
    'start': number;
    /**
     * The end time of the word in seconds from the beginning of the audio.
     * @type {number}
     * @memberof TranscriptionWord
     */
    'end': number;
    /**
     * The transcribed word.
     * @type {string}
     * @memberof TranscriptionWord
     */
    'word': string;
    /**
     * An identifier for the speaker of this word, present if diarization was enabled and successful.
     * @type {string}
     * @memberof TranscriptionWord
     */
    'speaker'?: string;
    /**
     * The model\'s confidence score for the transcription of this word, typically between 0 and 1 (if provided by the model).
     * @type {number}
     * @memberof TranscriptionWord
     */
    'confidence'?: number;
}

/**
 * OpenAICompatibleSpeechToTextApi - axios parameter creator
 * @export
 */
export const OpenAICompatibleSpeechToTextApiAxiosParamCreator = function (configuration?: Configuration) {
    return {
        /**
         * Mimics the OpenAI `/audio/transcriptions` endpoint. Accepts audio file uploads via `multipart/form-data`. Allows specifying model, language, prompt, response format, temperature, and timestamp granularity similar to OpenAI. Note: The `model` parameter should use Speechall\'s `provider.model` format. 
         * @summary Transcribes audio into the input language, using OpenAI-compatible request format.
         * @param {File} file The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm. 
         * @param {TranscriptionModelIdentifier} model 
         * @param {string} [language] The language of the input audio. Supplying the input language in [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) format will improve accuracy and latency. 
         * @param {string} [prompt] An optional text to guide the model\\\&#39;s style or continue a previous audio segment. The [prompt](/docs/guides/speech-to-text/prompting) should match the audio language. 
         * @param {OpenAIAudioResponseFormat} [responseFormat] 
         * @param {number} [temperature] The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit. 
         * @param {Array<OpenaiCompatibleCreateTranscriptionTimestampGranularitiesEnum>} [timestampGranularities] The timestamp granularities to populate for this transcription. &#x60;response_format&#x60; must be set &#x60;verbose_json&#x60; to use timestamp granularities. Either or both of these options are supported: &#x60;word&#x60;, or &#x60;segment&#x60;. Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency. 
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        openaiCompatibleCreateTranscription: async (file: File, model: TranscriptionModelIdentifier, language?: string, prompt?: string, responseFormat?: OpenAIAudioResponseFormat, temperature?: number, timestampGranularities?: Array<OpenaiCompatibleCreateTranscriptionTimestampGranularitiesEnum>, options: RawAxiosRequestConfig = {}): Promise<RequestArgs> => {
            // verify required parameter 'file' is not null or undefined
            assertParamExists('openaiCompatibleCreateTranscription', 'file', file)
            // verify required parameter 'model' is not null or undefined
            assertParamExists('openaiCompatibleCreateTranscription', 'model', model)
            const localVarPath = `/openai-compatible/audio/transcriptions`;
            // use dummy base URL string because the URL constructor only accepts absolute URLs.
            const localVarUrlObj = new URL(localVarPath, DUMMY_BASE_URL);
            let baseOptions;
            if (configuration) {
                baseOptions = configuration.baseOptions;
            }

            const localVarRequestOptions = { method: 'POST', ...baseOptions, ...options};
            const localVarHeaderParameter = {} as any;
            const localVarQueryParameter = {} as any;
            const localVarFormParams = new ((configuration && configuration.formDataCtor) || FormData)();

            // authentication bearerAuth required
            // http bearer authentication required
            await setBearerAuthToObject(localVarHeaderParameter, configuration)


            if (file !== undefined) { 
                localVarFormParams.append('file', file as any);
            }
    
            if (model !== undefined) { 
                localVarFormParams.append('model', model as any);
            }
    
            if (language !== undefined) { 
                localVarFormParams.append('language', language as any);
            }
    
            if (prompt !== undefined) { 
                localVarFormParams.append('prompt', prompt as any);
            }
    
            if (responseFormat !== undefined) { 
                localVarFormParams.append('response_format', responseFormat as any);
            }
    
            if (temperature !== undefined) { 
                localVarFormParams.append('temperature', temperature as any);
            }
                if (timestampGranularities) {
                localVarFormParams.append('timestamp_granularities[]', timestampGranularities.join(COLLECTION_FORMATS.csv));
            }

    
    
            localVarHeaderParameter['Content-Type'] = 'multipart/form-data';
    
            setSearchParams(localVarUrlObj, localVarQueryParameter);
            let headersFromBaseOptions = baseOptions && baseOptions.headers ? baseOptions.headers : {};
            localVarRequestOptions.headers = {...localVarHeaderParameter, ...headersFromBaseOptions, ...options.headers};
            localVarRequestOptions.data = localVarFormParams;

            return {
                url: toPathString(localVarUrlObj),
                options: localVarRequestOptions,
            };
        },
        /**
         * Mimics the OpenAI `/audio/translations` endpoint. Accepts audio file uploads via `multipart/form-data` and translates the speech into English text. Allows specifying model, prompt, response format, and temperature similar to OpenAI. Note: The `model` parameter should use Speechall\'s `provider.model` format (ensure the selected model supports translation). 
         * @summary Translates audio into English, using OpenAI-compatible request format.
         * @param {File} file The audio file object (not file name) translate, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm. 
         * @param {OpenAICreateTranslationRequestModel} model 
         * @param {string} [prompt] An optional text to guide the model\\\&#39;s style or continue a previous audio segment. The [prompt](/docs/guides/speech-to-text/prompting) should be in English. 
         * @param {OpenAIAudioResponseFormat} [responseFormat] 
         * @param {number} [temperature] The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit. 
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        openaiCompatibleCreateTranslation: async (file: File, model: OpenAICreateTranslationRequestModel, prompt?: string, responseFormat?: OpenAIAudioResponseFormat, temperature?: number, options: RawAxiosRequestConfig = {}): Promise<RequestArgs> => {
            // verify required parameter 'file' is not null or undefined
            assertParamExists('openaiCompatibleCreateTranslation', 'file', file)
            // verify required parameter 'model' is not null or undefined
            assertParamExists('openaiCompatibleCreateTranslation', 'model', model)
            const localVarPath = `/openai-compatible/audio/translations`;
            // use dummy base URL string because the URL constructor only accepts absolute URLs.
            const localVarUrlObj = new URL(localVarPath, DUMMY_BASE_URL);
            let baseOptions;
            if (configuration) {
                baseOptions = configuration.baseOptions;
            }

            const localVarRequestOptions = { method: 'POST', ...baseOptions, ...options};
            const localVarHeaderParameter = {} as any;
            const localVarQueryParameter = {} as any;
            const localVarFormParams = new ((configuration && configuration.formDataCtor) || FormData)();

            // authentication bearerAuth required
            // http bearer authentication required
            await setBearerAuthToObject(localVarHeaderParameter, configuration)


            if (file !== undefined) { 
                localVarFormParams.append('file', file as any);
            }
    
            if (model !== undefined) { 
                localVarFormParams.append('model', new Blob([JSON.stringify(model)], { type: "application/json", }));
            }
    
            if (prompt !== undefined) { 
                localVarFormParams.append('prompt', prompt as any);
            }
    
            if (responseFormat !== undefined) { 
                localVarFormParams.append('response_format', responseFormat as any);
            }
    
            if (temperature !== undefined) { 
                localVarFormParams.append('temperature', temperature as any);
            }
    
    
            localVarHeaderParameter['Content-Type'] = 'multipart/form-data';
    
            setSearchParams(localVarUrlObj, localVarQueryParameter);
            let headersFromBaseOptions = baseOptions && baseOptions.headers ? baseOptions.headers : {};
            localVarRequestOptions.headers = {...localVarHeaderParameter, ...headersFromBaseOptions, ...options.headers};
            localVarRequestOptions.data = localVarFormParams;

            return {
                url: toPathString(localVarUrlObj),
                options: localVarRequestOptions,
            };
        },
    }
};

/**
 * OpenAICompatibleSpeechToTextApi - functional programming interface
 * @export
 */
export const OpenAICompatibleSpeechToTextApiFp = function(configuration?: Configuration) {
    const localVarAxiosParamCreator = OpenAICompatibleSpeechToTextApiAxiosParamCreator(configuration)
    return {
        /**
         * Mimics the OpenAI `/audio/transcriptions` endpoint. Accepts audio file uploads via `multipart/form-data`. Allows specifying model, language, prompt, response format, temperature, and timestamp granularity similar to OpenAI. Note: The `model` parameter should use Speechall\'s `provider.model` format. 
         * @summary Transcribes audio into the input language, using OpenAI-compatible request format.
         * @param {File} file The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm. 
         * @param {TranscriptionModelIdentifier} model 
         * @param {string} [language] The language of the input audio. Supplying the input language in [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) format will improve accuracy and latency. 
         * @param {string} [prompt] An optional text to guide the model\\\&#39;s style or continue a previous audio segment. The [prompt](/docs/guides/speech-to-text/prompting) should match the audio language. 
         * @param {OpenAIAudioResponseFormat} [responseFormat] 
         * @param {number} [temperature] The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit. 
         * @param {Array<OpenaiCompatibleCreateTranscriptionTimestampGranularitiesEnum>} [timestampGranularities] The timestamp granularities to populate for this transcription. &#x60;response_format&#x60; must be set &#x60;verbose_json&#x60; to use timestamp granularities. Either or both of these options are supported: &#x60;word&#x60;, or &#x60;segment&#x60;. Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency. 
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        async openaiCompatibleCreateTranscription(file: File, model: TranscriptionModelIdentifier, language?: string, prompt?: string, responseFormat?: OpenAIAudioResponseFormat, temperature?: number, timestampGranularities?: Array<OpenaiCompatibleCreateTranscriptionTimestampGranularitiesEnum>, options?: RawAxiosRequestConfig): Promise<(axios?: AxiosInstance, basePath?: string) => AxiosPromise<OpenaiCompatibleCreateTranscription200Response>> {
            const localVarAxiosArgs = await localVarAxiosParamCreator.openaiCompatibleCreateTranscription(file, model, language, prompt, responseFormat, temperature, timestampGranularities, options);
            const localVarOperationServerIndex = configuration?.serverIndex ?? 0;
            const localVarOperationServerBasePath = operationServerMap['OpenAICompatibleSpeechToTextApi.openaiCompatibleCreateTranscription']?.[localVarOperationServerIndex]?.url;
            return (axios, basePath) => createRequestFunction(localVarAxiosArgs, globalAxios, BASE_PATH, configuration)(axios, localVarOperationServerBasePath || basePath);
        },
        /**
         * Mimics the OpenAI `/audio/translations` endpoint. Accepts audio file uploads via `multipart/form-data` and translates the speech into English text. Allows specifying model, prompt, response format, and temperature similar to OpenAI. Note: The `model` parameter should use Speechall\'s `provider.model` format (ensure the selected model supports translation). 
         * @summary Translates audio into English, using OpenAI-compatible request format.
         * @param {File} file The audio file object (not file name) translate, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm. 
         * @param {OpenAICreateTranslationRequestModel} model 
         * @param {string} [prompt] An optional text to guide the model\\\&#39;s style or continue a previous audio segment. The [prompt](/docs/guides/speech-to-text/prompting) should be in English. 
         * @param {OpenAIAudioResponseFormat} [responseFormat] 
         * @param {number} [temperature] The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit. 
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        async openaiCompatibleCreateTranslation(file: File, model: OpenAICreateTranslationRequestModel, prompt?: string, responseFormat?: OpenAIAudioResponseFormat, temperature?: number, options?: RawAxiosRequestConfig): Promise<(axios?: AxiosInstance, basePath?: string) => AxiosPromise<OpenaiCompatibleCreateTranslation200Response>> {
            const localVarAxiosArgs = await localVarAxiosParamCreator.openaiCompatibleCreateTranslation(file, model, prompt, responseFormat, temperature, options);
            const localVarOperationServerIndex = configuration?.serverIndex ?? 0;
            const localVarOperationServerBasePath = operationServerMap['OpenAICompatibleSpeechToTextApi.openaiCompatibleCreateTranslation']?.[localVarOperationServerIndex]?.url;
            return (axios, basePath) => createRequestFunction(localVarAxiosArgs, globalAxios, BASE_PATH, configuration)(axios, localVarOperationServerBasePath || basePath);
        },
    }
};

/**
 * OpenAICompatibleSpeechToTextApi - factory interface
 * @export
 */
export const OpenAICompatibleSpeechToTextApiFactory = function (configuration?: Configuration, basePath?: string, axios?: AxiosInstance) {
    const localVarFp = OpenAICompatibleSpeechToTextApiFp(configuration)
    return {
        /**
         * Mimics the OpenAI `/audio/transcriptions` endpoint. Accepts audio file uploads via `multipart/form-data`. Allows specifying model, language, prompt, response format, temperature, and timestamp granularity similar to OpenAI. Note: The `model` parameter should use Speechall\'s `provider.model` format. 
         * @summary Transcribes audio into the input language, using OpenAI-compatible request format.
         * @param {File} file The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm. 
         * @param {TranscriptionModelIdentifier} model 
         * @param {string} [language] The language of the input audio. Supplying the input language in [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) format will improve accuracy and latency. 
         * @param {string} [prompt] An optional text to guide the model\\\&#39;s style or continue a previous audio segment. The [prompt](/docs/guides/speech-to-text/prompting) should match the audio language. 
         * @param {OpenAIAudioResponseFormat} [responseFormat] 
         * @param {number} [temperature] The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit. 
         * @param {Array<OpenaiCompatibleCreateTranscriptionTimestampGranularitiesEnum>} [timestampGranularities] The timestamp granularities to populate for this transcription. &#x60;response_format&#x60; must be set &#x60;verbose_json&#x60; to use timestamp granularities. Either or both of these options are supported: &#x60;word&#x60;, or &#x60;segment&#x60;. Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency. 
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        openaiCompatibleCreateTranscription(file: File, model: TranscriptionModelIdentifier, language?: string, prompt?: string, responseFormat?: OpenAIAudioResponseFormat, temperature?: number, timestampGranularities?: Array<OpenaiCompatibleCreateTranscriptionTimestampGranularitiesEnum>, options?: RawAxiosRequestConfig): AxiosPromise<OpenaiCompatibleCreateTranscription200Response> {
            return localVarFp.openaiCompatibleCreateTranscription(file, model, language, prompt, responseFormat, temperature, timestampGranularities, options).then((request) => request(axios, basePath));
        },
        /**
         * Mimics the OpenAI `/audio/translations` endpoint. Accepts audio file uploads via `multipart/form-data` and translates the speech into English text. Allows specifying model, prompt, response format, and temperature similar to OpenAI. Note: The `model` parameter should use Speechall\'s `provider.model` format (ensure the selected model supports translation). 
         * @summary Translates audio into English, using OpenAI-compatible request format.
         * @param {File} file The audio file object (not file name) translate, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm. 
         * @param {OpenAICreateTranslationRequestModel} model 
         * @param {string} [prompt] An optional text to guide the model\\\&#39;s style or continue a previous audio segment. The [prompt](/docs/guides/speech-to-text/prompting) should be in English. 
         * @param {OpenAIAudioResponseFormat} [responseFormat] 
         * @param {number} [temperature] The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit. 
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        openaiCompatibleCreateTranslation(file: File, model: OpenAICreateTranslationRequestModel, prompt?: string, responseFormat?: OpenAIAudioResponseFormat, temperature?: number, options?: RawAxiosRequestConfig): AxiosPromise<OpenaiCompatibleCreateTranslation200Response> {
            return localVarFp.openaiCompatibleCreateTranslation(file, model, prompt, responseFormat, temperature, options).then((request) => request(axios, basePath));
        },
    };
};

/**
 * OpenAICompatibleSpeechToTextApi - object-oriented interface
 * @export
 * @class OpenAICompatibleSpeechToTextApi
 * @extends {BaseAPI}
 */
export class OpenAICompatibleSpeechToTextApi extends BaseAPI {
    /**
     * Mimics the OpenAI `/audio/transcriptions` endpoint. Accepts audio file uploads via `multipart/form-data`. Allows specifying model, language, prompt, response format, temperature, and timestamp granularity similar to OpenAI. Note: The `model` parameter should use Speechall\'s `provider.model` format. 
     * @summary Transcribes audio into the input language, using OpenAI-compatible request format.
     * @param {File} file The audio file object (not file name) to transcribe, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm. 
     * @param {TranscriptionModelIdentifier} model 
     * @param {string} [language] The language of the input audio. Supplying the input language in [ISO-639-1](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) format will improve accuracy and latency. 
     * @param {string} [prompt] An optional text to guide the model\\\&#39;s style or continue a previous audio segment. The [prompt](/docs/guides/speech-to-text/prompting) should match the audio language. 
     * @param {OpenAIAudioResponseFormat} [responseFormat] 
     * @param {number} [temperature] The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit. 
     * @param {Array<OpenaiCompatibleCreateTranscriptionTimestampGranularitiesEnum>} [timestampGranularities] The timestamp granularities to populate for this transcription. &#x60;response_format&#x60; must be set &#x60;verbose_json&#x60; to use timestamp granularities. Either or both of these options are supported: &#x60;word&#x60;, or &#x60;segment&#x60;. Note: There is no additional latency for segment timestamps, but generating word timestamps incurs additional latency. 
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof OpenAICompatibleSpeechToTextApi
     */
    public openaiCompatibleCreateTranscription(file: File, model: TranscriptionModelIdentifier, language?: string, prompt?: string, responseFormat?: OpenAIAudioResponseFormat, temperature?: number, timestampGranularities?: Array<OpenaiCompatibleCreateTranscriptionTimestampGranularitiesEnum>, options?: RawAxiosRequestConfig) {
        return OpenAICompatibleSpeechToTextApiFp(this.configuration).openaiCompatibleCreateTranscription(file, model, language, prompt, responseFormat, temperature, timestampGranularities, options).then((request) => request(this.axios, this.basePath));
    }

    /**
     * Mimics the OpenAI `/audio/translations` endpoint. Accepts audio file uploads via `multipart/form-data` and translates the speech into English text. Allows specifying model, prompt, response format, and temperature similar to OpenAI. Note: The `model` parameter should use Speechall\'s `provider.model` format (ensure the selected model supports translation). 
     * @summary Translates audio into English, using OpenAI-compatible request format.
     * @param {File} file The audio file object (not file name) translate, in one of these formats: flac, mp3, mp4, mpeg, mpga, m4a, ogg, wav, or webm. 
     * @param {OpenAICreateTranslationRequestModel} model 
     * @param {string} [prompt] An optional text to guide the model\\\&#39;s style or continue a previous audio segment. The [prompt](/docs/guides/speech-to-text/prompting) should be in English. 
     * @param {OpenAIAudioResponseFormat} [responseFormat] 
     * @param {number} [temperature] The sampling temperature, between 0 and 1. Higher values like 0.8 will make the output more random, while lower values like 0.2 will make it more focused and deterministic. If set to 0, the model will use [log probability](https://en.wikipedia.org/wiki/Log_probability) to automatically increase the temperature until certain thresholds are hit. 
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof OpenAICompatibleSpeechToTextApi
     */
    public openaiCompatibleCreateTranslation(file: File, model: OpenAICreateTranslationRequestModel, prompt?: string, responseFormat?: OpenAIAudioResponseFormat, temperature?: number, options?: RawAxiosRequestConfig) {
        return OpenAICompatibleSpeechToTextApiFp(this.configuration).openaiCompatibleCreateTranslation(file, model, prompt, responseFormat, temperature, options).then((request) => request(this.axios, this.basePath));
    }
}

/**
 * @export
 */
export const OpenaiCompatibleCreateTranscriptionTimestampGranularitiesEnum = {
    Word: 'word',
    Segment: 'segment'
} as const;
export type OpenaiCompatibleCreateTranscriptionTimestampGranularitiesEnum = typeof OpenaiCompatibleCreateTranscriptionTimestampGranularitiesEnum[keyof typeof OpenaiCompatibleCreateTranscriptionTimestampGranularitiesEnum];


/**
 * ReplacementRulesApi - axios parameter creator
 * @export
 */
export const ReplacementRulesApiAxiosParamCreator = function (configuration?: Configuration) {
    return {
        /**
         * Defines a named set of replacement rules (exact match, regex) that can be applied during transcription requests using its `ruleset_id`. Rules within a set are applied sequentially to the transcription text. 
         * @summary Create a reusable set of text replacement rules.
         * @param {CreateReplacementRulesetRequest} createReplacementRulesetRequest JSON object containing the name for the ruleset and an array of replacement rule objects.
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        createReplacementRuleset: async (createReplacementRulesetRequest: CreateReplacementRulesetRequest, options: RawAxiosRequestConfig = {}): Promise<RequestArgs> => {
            // verify required parameter 'createReplacementRulesetRequest' is not null or undefined
            assertParamExists('createReplacementRuleset', 'createReplacementRulesetRequest', createReplacementRulesetRequest)
            const localVarPath = `/replacement-rulesets`;
            // use dummy base URL string because the URL constructor only accepts absolute URLs.
            const localVarUrlObj = new URL(localVarPath, DUMMY_BASE_URL);
            let baseOptions;
            if (configuration) {
                baseOptions = configuration.baseOptions;
            }

            const localVarRequestOptions = { method: 'POST', ...baseOptions, ...options};
            const localVarHeaderParameter = {} as any;
            const localVarQueryParameter = {} as any;

            // authentication bearerAuth required
            // http bearer authentication required
            await setBearerAuthToObject(localVarHeaderParameter, configuration)


    
            localVarHeaderParameter['Content-Type'] = 'application/json';

            setSearchParams(localVarUrlObj, localVarQueryParameter);
            let headersFromBaseOptions = baseOptions && baseOptions.headers ? baseOptions.headers : {};
            localVarRequestOptions.headers = {...localVarHeaderParameter, ...headersFromBaseOptions, ...options.headers};
            localVarRequestOptions.data = serializeDataIfNeeded(createReplacementRulesetRequest, localVarRequestOptions, configuration)

            return {
                url: toPathString(localVarUrlObj),
                options: localVarRequestOptions,
            };
        },
    }
};

/**
 * ReplacementRulesApi - functional programming interface
 * @export
 */
export const ReplacementRulesApiFp = function(configuration?: Configuration) {
    const localVarAxiosParamCreator = ReplacementRulesApiAxiosParamCreator(configuration)
    return {
        /**
         * Defines a named set of replacement rules (exact match, regex) that can be applied during transcription requests using its `ruleset_id`. Rules within a set are applied sequentially to the transcription text. 
         * @summary Create a reusable set of text replacement rules.
         * @param {CreateReplacementRulesetRequest} createReplacementRulesetRequest JSON object containing the name for the ruleset and an array of replacement rule objects.
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        async createReplacementRuleset(createReplacementRulesetRequest: CreateReplacementRulesetRequest, options?: RawAxiosRequestConfig): Promise<(axios?: AxiosInstance, basePath?: string) => AxiosPromise<CreateReplacementRuleset201Response>> {
            const localVarAxiosArgs = await localVarAxiosParamCreator.createReplacementRuleset(createReplacementRulesetRequest, options);
            const localVarOperationServerIndex = configuration?.serverIndex ?? 0;
            const localVarOperationServerBasePath = operationServerMap['ReplacementRulesApi.createReplacementRuleset']?.[localVarOperationServerIndex]?.url;
            return (axios, basePath) => createRequestFunction(localVarAxiosArgs, globalAxios, BASE_PATH, configuration)(axios, localVarOperationServerBasePath || basePath);
        },
    }
};

/**
 * ReplacementRulesApi - factory interface
 * @export
 */
export const ReplacementRulesApiFactory = function (configuration?: Configuration, basePath?: string, axios?: AxiosInstance) {
    const localVarFp = ReplacementRulesApiFp(configuration)
    return {
        /**
         * Defines a named set of replacement rules (exact match, regex) that can be applied during transcription requests using its `ruleset_id`. Rules within a set are applied sequentially to the transcription text. 
         * @summary Create a reusable set of text replacement rules.
         * @param {CreateReplacementRulesetRequest} createReplacementRulesetRequest JSON object containing the name for the ruleset and an array of replacement rule objects.
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        createReplacementRuleset(createReplacementRulesetRequest: CreateReplacementRulesetRequest, options?: RawAxiosRequestConfig): AxiosPromise<CreateReplacementRuleset201Response> {
            return localVarFp.createReplacementRuleset(createReplacementRulesetRequest, options).then((request) => request(axios, basePath));
        },
    };
};

/**
 * ReplacementRulesApi - object-oriented interface
 * @export
 * @class ReplacementRulesApi
 * @extends {BaseAPI}
 */
export class ReplacementRulesApi extends BaseAPI {
    /**
     * Defines a named set of replacement rules (exact match, regex) that can be applied during transcription requests using its `ruleset_id`. Rules within a set are applied sequentially to the transcription text. 
     * @summary Create a reusable set of text replacement rules.
     * @param {CreateReplacementRulesetRequest} createReplacementRulesetRequest JSON object containing the name for the ruleset and an array of replacement rule objects.
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof ReplacementRulesApi
     */
    public createReplacementRuleset(createReplacementRulesetRequest: CreateReplacementRulesetRequest, options?: RawAxiosRequestConfig) {
        return ReplacementRulesApiFp(this.configuration).createReplacementRuleset(createReplacementRulesetRequest, options).then((request) => request(this.axios, this.basePath));
    }
}



/**
 * SpeechToTextApi - axios parameter creator
 * @export
 */
export const SpeechToTextApiAxiosParamCreator = function (configuration?: Configuration) {
    return {
        /**
         * Returns a detailed list of all STT models accessible through the Speechall API. Each model entry includes its identifier (`provider.model`), display name, description, supported features (languages, formats, punctuation, diarization), and performance characteristics. Use this endpoint to discover available models and their capabilities before making transcription requests. 
         * @summary Retrieve a list of all available speech-to-text models.
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        listSpeechToTextModels: async (options: RawAxiosRequestConfig = {}): Promise<RequestArgs> => {
            const localVarPath = `/speech-to-text-models`;
            // use dummy base URL string because the URL constructor only accepts absolute URLs.
            const localVarUrlObj = new URL(localVarPath, DUMMY_BASE_URL);
            let baseOptions;
            if (configuration) {
                baseOptions = configuration.baseOptions;
            }

            const localVarRequestOptions = { method: 'GET', ...baseOptions, ...options};
            const localVarHeaderParameter = {} as any;
            const localVarQueryParameter = {} as any;

            // authentication bearerAuth required
            // http bearer authentication required
            await setBearerAuthToObject(localVarHeaderParameter, configuration)


    
            setSearchParams(localVarUrlObj, localVarQueryParameter);
            let headersFromBaseOptions = baseOptions && baseOptions.headers ? baseOptions.headers : {};
            localVarRequestOptions.headers = {...localVarHeaderParameter, ...headersFromBaseOptions, ...options.headers};

            return {
                url: toPathString(localVarUrlObj),
                options: localVarRequestOptions,
            };
        },
        /**
         * This endpoint allows you to send raw audio data in the request body for transcription. You can specify the desired model, language, output format, and various provider-specific features using query parameters. Suitable for transcribing local audio files. 
         * @summary Upload an audio file directly and receive a transcription.
         * @param {TranscriptionModelIdentifier} model The identifier of the speech-to-text model to use for the transcription, in the format &#x60;provider.model&#x60;. See the &#x60;/speech-to-text-models&#x60; endpoint for available models.
         * @param {File} body The audio file to transcribe. Send the raw audio data as the request body. Supported formats typically include WAV, MP3, FLAC, Ogg, M4A, etc., depending on the chosen model/provider. Check provider documentation for specific limits on file size and duration.
         * @param {TranscriptLanguageCode} [language] The language of the audio file in ISO 639-1 format (e.g., &#x60;en&#x60;, &#x60;es&#x60;, &#x60;fr&#x60;). Specify &#x60;auto&#x60; for automatic language detection (if supported by the model). Defaults to &#x60;en&#x60; if not provided. Providing the correct language improves accuracy and latency.
         * @param {TranscriptOutputFormat} [outputFormat] The desired format for the transcription output. Can be plain text, JSON objects (simple or detailed), or subtitle formats (SRT, VTT). Defaults to &#x60;text&#x60;.
         * @param {string} [rulesetId] The unique identifier (UUID) of a pre-defined replacement ruleset to apply to the final transcription text. Create rulesets using the &#x60;/replacement-rulesets&#x60; endpoint.
         * @param {boolean} [punctuation] Enable automatic punctuation (commas, periods, question marks) in the transcription. Support varies by model/provider (e.g., Deepgram, AssemblyAI). Defaults to &#x60;true&#x60;.
         * @param {TranscribeTimestampGranularityEnum} [timestampGranularity] Specifies the level of detail for timestamps in the response (if &#x60;output_format&#x60; is &#x60;json&#x60; or &#x60;verbose_json&#x60;). &#x60;segment&#x60; provides timestamps for larger chunks of speech, while &#x60;word&#x60; provides timestamps for individual words (may increase latency). Defaults to &#x60;segment&#x60;.
         * @param {boolean} [diarization] Enable speaker diarization to identify and label different speakers in the audio. Support and quality vary by model/provider. Defaults to &#x60;false&#x60;. When enabled, the &#x60;speaker&#x60; field may be populated in the response segments.
         * @param {string} [initialPrompt] An optional text prompt to provide context, guide the model\&#39;s style (e.g., spelling of specific names), or improve accuracy for subsequent audio segments. Support varies by model (e.g., OpenAI models).
         * @param {number} [temperature] Controls the randomness of the output for certain models (e.g., OpenAI). A value between 0 and 1. Lower values (e.g., 0.2) make the output more deterministic, while higher values (e.g., 0.8) make it more random. Defaults vary by model.
         * @param {boolean} [smartFormat] Enable provider-specific \&quot;smart formatting\&quot; features, which might include formatting for numbers, dates, currency, etc. Currently supported by Deepgram models. Defaults vary.
         * @param {number} [speakersExpected] Provides a hint to the diarization process about the number of expected speakers. May improve accuracy for some providers (e.g., RevAI, Deepgram).
         * @param {Array<string>} [customVocabulary] Provide a list of specific words or phrases (e.g., proper nouns, jargon) to increase their recognition likelihood. Support varies by provider (e.g., Deepgram, AssemblyAI).
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        transcribe: async (model: TranscriptionModelIdentifier, body: File, language?: TranscriptLanguageCode, outputFormat?: TranscriptOutputFormat, rulesetId?: string, punctuation?: boolean, timestampGranularity?: TranscribeTimestampGranularityEnum, diarization?: boolean, initialPrompt?: string, temperature?: number, smartFormat?: boolean, speakersExpected?: number, customVocabulary?: Array<string>, options: RawAxiosRequestConfig = {}): Promise<RequestArgs> => {
            // verify required parameter 'model' is not null or undefined
            assertParamExists('transcribe', 'model', model)
            // verify required parameter 'body' is not null or undefined
            assertParamExists('transcribe', 'body', body)
            const localVarPath = `/transcribe`;
            // use dummy base URL string because the URL constructor only accepts absolute URLs.
            const localVarUrlObj = new URL(localVarPath, DUMMY_BASE_URL);
            let baseOptions;
            if (configuration) {
                baseOptions = configuration.baseOptions;
            }

            const localVarRequestOptions = { method: 'POST', ...baseOptions, ...options};
            const localVarHeaderParameter = {} as any;
            const localVarQueryParameter = {} as any;

            // authentication bearerAuth required
            // http bearer authentication required
            await setBearerAuthToObject(localVarHeaderParameter, configuration)

            if (model !== undefined) {
                localVarQueryParameter['model'] = model;
            }

            if (language !== undefined) {
                localVarQueryParameter['language'] = language;
            }

            if (outputFormat !== undefined) {
                localVarQueryParameter['output_format'] = outputFormat;
            }

            if (rulesetId !== undefined) {
                localVarQueryParameter['ruleset_id'] = rulesetId;
            }

            if (punctuation !== undefined) {
                localVarQueryParameter['punctuation'] = punctuation;
            }

            if (timestampGranularity !== undefined) {
                localVarQueryParameter['timestamp_granularity'] = timestampGranularity;
            }

            if (diarization !== undefined) {
                localVarQueryParameter['diarization'] = diarization;
            }

            if (initialPrompt !== undefined) {
                localVarQueryParameter['initial_prompt'] = initialPrompt;
            }

            if (temperature !== undefined) {
                localVarQueryParameter['temperature'] = temperature;
            }

            if (smartFormat !== undefined) {
                localVarQueryParameter['smart_format'] = smartFormat;
            }

            if (speakersExpected !== undefined) {
                localVarQueryParameter['speakers_expected'] = speakersExpected;
            }

            if (customVocabulary) {
                localVarQueryParameter['custom_vocabulary'] = customVocabulary;
            }


    
            localVarHeaderParameter['Content-Type'] = 'audio/*';

            setSearchParams(localVarUrlObj, localVarQueryParameter);
            let headersFromBaseOptions = baseOptions && baseOptions.headers ? baseOptions.headers : {};
            localVarRequestOptions.headers = {...localVarHeaderParameter, ...headersFromBaseOptions, ...options.headers};
            localVarRequestOptions.data = serializeDataIfNeeded(body, localVarRequestOptions, configuration)

            return {
                url: toPathString(localVarUrlObj),
                options: localVarRequestOptions,
            };
        },
        /**
         * This endpoint allows you to transcribe an audio file hosted at a publicly accessible URL. Provide the URL and transcription options within the JSON request body. Useful for transcribing files already stored online. 
         * @summary Transcribe an audio file located at a remote URL.
         * @param {RemoteTranscriptionConfiguration} remoteTranscriptionConfiguration JSON object containing the URL of the audio file and the desired transcription options.
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        transcribeRemote: async (remoteTranscriptionConfiguration: RemoteTranscriptionConfiguration, options: RawAxiosRequestConfig = {}): Promise<RequestArgs> => {
            // verify required parameter 'remoteTranscriptionConfiguration' is not null or undefined
            assertParamExists('transcribeRemote', 'remoteTranscriptionConfiguration', remoteTranscriptionConfiguration)
            const localVarPath = `/transcribe-remote`;
            // use dummy base URL string because the URL constructor only accepts absolute URLs.
            const localVarUrlObj = new URL(localVarPath, DUMMY_BASE_URL);
            let baseOptions;
            if (configuration) {
                baseOptions = configuration.baseOptions;
            }

            const localVarRequestOptions = { method: 'POST', ...baseOptions, ...options};
            const localVarHeaderParameter = {} as any;
            const localVarQueryParameter = {} as any;

            // authentication bearerAuth required
            // http bearer authentication required
            await setBearerAuthToObject(localVarHeaderParameter, configuration)


    
            localVarHeaderParameter['Content-Type'] = 'application/json';

            setSearchParams(localVarUrlObj, localVarQueryParameter);
            let headersFromBaseOptions = baseOptions && baseOptions.headers ? baseOptions.headers : {};
            localVarRequestOptions.headers = {...localVarHeaderParameter, ...headersFromBaseOptions, ...options.headers};
            localVarRequestOptions.data = serializeDataIfNeeded(remoteTranscriptionConfiguration, localVarRequestOptions, configuration)

            return {
                url: toPathString(localVarUrlObj),
                options: localVarRequestOptions,
            };
        },
    }
};

/**
 * SpeechToTextApi - functional programming interface
 * @export
 */
export const SpeechToTextApiFp = function(configuration?: Configuration) {
    const localVarAxiosParamCreator = SpeechToTextApiAxiosParamCreator(configuration)
    return {
        /**
         * Returns a detailed list of all STT models accessible through the Speechall API. Each model entry includes its identifier (`provider.model`), display name, description, supported features (languages, formats, punctuation, diarization), and performance characteristics. Use this endpoint to discover available models and their capabilities before making transcription requests. 
         * @summary Retrieve a list of all available speech-to-text models.
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        async listSpeechToTextModels(options?: RawAxiosRequestConfig): Promise<(axios?: AxiosInstance, basePath?: string) => AxiosPromise<Array<SpeechToTextModel>>> {
            const localVarAxiosArgs = await localVarAxiosParamCreator.listSpeechToTextModels(options);
            const localVarOperationServerIndex = configuration?.serverIndex ?? 0;
            const localVarOperationServerBasePath = operationServerMap['SpeechToTextApi.listSpeechToTextModels']?.[localVarOperationServerIndex]?.url;
            return (axios, basePath) => createRequestFunction(localVarAxiosArgs, globalAxios, BASE_PATH, configuration)(axios, localVarOperationServerBasePath || basePath);
        },
        /**
         * This endpoint allows you to send raw audio data in the request body for transcription. You can specify the desired model, language, output format, and various provider-specific features using query parameters. Suitable for transcribing local audio files. 
         * @summary Upload an audio file directly and receive a transcription.
         * @param {TranscriptionModelIdentifier} model The identifier of the speech-to-text model to use for the transcription, in the format &#x60;provider.model&#x60;. See the &#x60;/speech-to-text-models&#x60; endpoint for available models.
         * @param {File} body The audio file to transcribe. Send the raw audio data as the request body. Supported formats typically include WAV, MP3, FLAC, Ogg, M4A, etc., depending on the chosen model/provider. Check provider documentation for specific limits on file size and duration.
         * @param {TranscriptLanguageCode} [language] The language of the audio file in ISO 639-1 format (e.g., &#x60;en&#x60;, &#x60;es&#x60;, &#x60;fr&#x60;). Specify &#x60;auto&#x60; for automatic language detection (if supported by the model). Defaults to &#x60;en&#x60; if not provided. Providing the correct language improves accuracy and latency.
         * @param {TranscriptOutputFormat} [outputFormat] The desired format for the transcription output. Can be plain text, JSON objects (simple or detailed), or subtitle formats (SRT, VTT). Defaults to &#x60;text&#x60;.
         * @param {string} [rulesetId] The unique identifier (UUID) of a pre-defined replacement ruleset to apply to the final transcription text. Create rulesets using the &#x60;/replacement-rulesets&#x60; endpoint.
         * @param {boolean} [punctuation] Enable automatic punctuation (commas, periods, question marks) in the transcription. Support varies by model/provider (e.g., Deepgram, AssemblyAI). Defaults to &#x60;true&#x60;.
         * @param {TranscribeTimestampGranularityEnum} [timestampGranularity] Specifies the level of detail for timestamps in the response (if &#x60;output_format&#x60; is &#x60;json&#x60; or &#x60;verbose_json&#x60;). &#x60;segment&#x60; provides timestamps for larger chunks of speech, while &#x60;word&#x60; provides timestamps for individual words (may increase latency). Defaults to &#x60;segment&#x60;.
         * @param {boolean} [diarization] Enable speaker diarization to identify and label different speakers in the audio. Support and quality vary by model/provider. Defaults to &#x60;false&#x60;. When enabled, the &#x60;speaker&#x60; field may be populated in the response segments.
         * @param {string} [initialPrompt] An optional text prompt to provide context, guide the model\&#39;s style (e.g., spelling of specific names), or improve accuracy for subsequent audio segments. Support varies by model (e.g., OpenAI models).
         * @param {number} [temperature] Controls the randomness of the output for certain models (e.g., OpenAI). A value between 0 and 1. Lower values (e.g., 0.2) make the output more deterministic, while higher values (e.g., 0.8) make it more random. Defaults vary by model.
         * @param {boolean} [smartFormat] Enable provider-specific \&quot;smart formatting\&quot; features, which might include formatting for numbers, dates, currency, etc. Currently supported by Deepgram models. Defaults vary.
         * @param {number} [speakersExpected] Provides a hint to the diarization process about the number of expected speakers. May improve accuracy for some providers (e.g., RevAI, Deepgram).
         * @param {Array<string>} [customVocabulary] Provide a list of specific words or phrases (e.g., proper nouns, jargon) to increase their recognition likelihood. Support varies by provider (e.g., Deepgram, AssemblyAI).
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        async transcribe(model: TranscriptionModelIdentifier, body: File, language?: TranscriptLanguageCode, outputFormat?: TranscriptOutputFormat, rulesetId?: string, punctuation?: boolean, timestampGranularity?: TranscribeTimestampGranularityEnum, diarization?: boolean, initialPrompt?: string, temperature?: number, smartFormat?: boolean, speakersExpected?: number, customVocabulary?: Array<string>, options?: RawAxiosRequestConfig): Promise<(axios?: AxiosInstance, basePath?: string) => AxiosPromise<TranscriptionResponse>> {
            const localVarAxiosArgs = await localVarAxiosParamCreator.transcribe(model, body, language, outputFormat, rulesetId, punctuation, timestampGranularity, diarization, initialPrompt, temperature, smartFormat, speakersExpected, customVocabulary, options);
            const localVarOperationServerIndex = configuration?.serverIndex ?? 0;
            const localVarOperationServerBasePath = operationServerMap['SpeechToTextApi.transcribe']?.[localVarOperationServerIndex]?.url;
            return (axios, basePath) => createRequestFunction(localVarAxiosArgs, globalAxios, BASE_PATH, configuration)(axios, localVarOperationServerBasePath || basePath);
        },
        /**
         * This endpoint allows you to transcribe an audio file hosted at a publicly accessible URL. Provide the URL and transcription options within the JSON request body. Useful for transcribing files already stored online. 
         * @summary Transcribe an audio file located at a remote URL.
         * @param {RemoteTranscriptionConfiguration} remoteTranscriptionConfiguration JSON object containing the URL of the audio file and the desired transcription options.
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        async transcribeRemote(remoteTranscriptionConfiguration: RemoteTranscriptionConfiguration, options?: RawAxiosRequestConfig): Promise<(axios?: AxiosInstance, basePath?: string) => AxiosPromise<TranscriptionResponse>> {
            const localVarAxiosArgs = await localVarAxiosParamCreator.transcribeRemote(remoteTranscriptionConfiguration, options);
            const localVarOperationServerIndex = configuration?.serverIndex ?? 0;
            const localVarOperationServerBasePath = operationServerMap['SpeechToTextApi.transcribeRemote']?.[localVarOperationServerIndex]?.url;
            return (axios, basePath) => createRequestFunction(localVarAxiosArgs, globalAxios, BASE_PATH, configuration)(axios, localVarOperationServerBasePath || basePath);
        },
    }
};

/**
 * SpeechToTextApi - factory interface
 * @export
 */
export const SpeechToTextApiFactory = function (configuration?: Configuration, basePath?: string, axios?: AxiosInstance) {
    const localVarFp = SpeechToTextApiFp(configuration)
    return {
        /**
         * Returns a detailed list of all STT models accessible through the Speechall API. Each model entry includes its identifier (`provider.model`), display name, description, supported features (languages, formats, punctuation, diarization), and performance characteristics. Use this endpoint to discover available models and their capabilities before making transcription requests. 
         * @summary Retrieve a list of all available speech-to-text models.
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        listSpeechToTextModels(options?: RawAxiosRequestConfig): AxiosPromise<Array<SpeechToTextModel>> {
            return localVarFp.listSpeechToTextModels(options).then((request) => request(axios, basePath));
        },
        /**
         * This endpoint allows you to send raw audio data in the request body for transcription. You can specify the desired model, language, output format, and various provider-specific features using query parameters. Suitable for transcribing local audio files. 
         * @summary Upload an audio file directly and receive a transcription.
         * @param {TranscriptionModelIdentifier} model The identifier of the speech-to-text model to use for the transcription, in the format &#x60;provider.model&#x60;. See the &#x60;/speech-to-text-models&#x60; endpoint for available models.
         * @param {File} body The audio file to transcribe. Send the raw audio data as the request body. Supported formats typically include WAV, MP3, FLAC, Ogg, M4A, etc., depending on the chosen model/provider. Check provider documentation for specific limits on file size and duration.
         * @param {TranscriptLanguageCode} [language] The language of the audio file in ISO 639-1 format (e.g., &#x60;en&#x60;, &#x60;es&#x60;, &#x60;fr&#x60;). Specify &#x60;auto&#x60; for automatic language detection (if supported by the model). Defaults to &#x60;en&#x60; if not provided. Providing the correct language improves accuracy and latency.
         * @param {TranscriptOutputFormat} [outputFormat] The desired format for the transcription output. Can be plain text, JSON objects (simple or detailed), or subtitle formats (SRT, VTT). Defaults to &#x60;text&#x60;.
         * @param {string} [rulesetId] The unique identifier (UUID) of a pre-defined replacement ruleset to apply to the final transcription text. Create rulesets using the &#x60;/replacement-rulesets&#x60; endpoint.
         * @param {boolean} [punctuation] Enable automatic punctuation (commas, periods, question marks) in the transcription. Support varies by model/provider (e.g., Deepgram, AssemblyAI). Defaults to &#x60;true&#x60;.
         * @param {TranscribeTimestampGranularityEnum} [timestampGranularity] Specifies the level of detail for timestamps in the response (if &#x60;output_format&#x60; is &#x60;json&#x60; or &#x60;verbose_json&#x60;). &#x60;segment&#x60; provides timestamps for larger chunks of speech, while &#x60;word&#x60; provides timestamps for individual words (may increase latency). Defaults to &#x60;segment&#x60;.
         * @param {boolean} [diarization] Enable speaker diarization to identify and label different speakers in the audio. Support and quality vary by model/provider. Defaults to &#x60;false&#x60;. When enabled, the &#x60;speaker&#x60; field may be populated in the response segments.
         * @param {string} [initialPrompt] An optional text prompt to provide context, guide the model\&#39;s style (e.g., spelling of specific names), or improve accuracy for subsequent audio segments. Support varies by model (e.g., OpenAI models).
         * @param {number} [temperature] Controls the randomness of the output for certain models (e.g., OpenAI). A value between 0 and 1. Lower values (e.g., 0.2) make the output more deterministic, while higher values (e.g., 0.8) make it more random. Defaults vary by model.
         * @param {boolean} [smartFormat] Enable provider-specific \&quot;smart formatting\&quot; features, which might include formatting for numbers, dates, currency, etc. Currently supported by Deepgram models. Defaults vary.
         * @param {number} [speakersExpected] Provides a hint to the diarization process about the number of expected speakers. May improve accuracy for some providers (e.g., RevAI, Deepgram).
         * @param {Array<string>} [customVocabulary] Provide a list of specific words or phrases (e.g., proper nouns, jargon) to increase their recognition likelihood. Support varies by provider (e.g., Deepgram, AssemblyAI).
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        transcribe(model: TranscriptionModelIdentifier, body: File, language?: TranscriptLanguageCode, outputFormat?: TranscriptOutputFormat, rulesetId?: string, punctuation?: boolean, timestampGranularity?: TranscribeTimestampGranularityEnum, diarization?: boolean, initialPrompt?: string, temperature?: number, smartFormat?: boolean, speakersExpected?: number, customVocabulary?: Array<string>, options?: RawAxiosRequestConfig): AxiosPromise<TranscriptionResponse> {
            return localVarFp.transcribe(model, body, language, outputFormat, rulesetId, punctuation, timestampGranularity, diarization, initialPrompt, temperature, smartFormat, speakersExpected, customVocabulary, options).then((request) => request(axios, basePath));
        },
        /**
         * This endpoint allows you to transcribe an audio file hosted at a publicly accessible URL. Provide the URL and transcription options within the JSON request body. Useful for transcribing files already stored online. 
         * @summary Transcribe an audio file located at a remote URL.
         * @param {RemoteTranscriptionConfiguration} remoteTranscriptionConfiguration JSON object containing the URL of the audio file and the desired transcription options.
         * @param {*} [options] Override http request option.
         * @throws {RequiredError}
         */
        transcribeRemote(remoteTranscriptionConfiguration: RemoteTranscriptionConfiguration, options?: RawAxiosRequestConfig): AxiosPromise<TranscriptionResponse> {
            return localVarFp.transcribeRemote(remoteTranscriptionConfiguration, options).then((request) => request(axios, basePath));
        },
    };
};

/**
 * SpeechToTextApi - object-oriented interface
 * @export
 * @class SpeechToTextApi
 * @extends {BaseAPI}
 */
export class SpeechToTextApi extends BaseAPI {
    /**
     * Returns a detailed list of all STT models accessible through the Speechall API. Each model entry includes its identifier (`provider.model`), display name, description, supported features (languages, formats, punctuation, diarization), and performance characteristics. Use this endpoint to discover available models and their capabilities before making transcription requests. 
     * @summary Retrieve a list of all available speech-to-text models.
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof SpeechToTextApi
     */
    public listSpeechToTextModels(options?: RawAxiosRequestConfig) {
        return SpeechToTextApiFp(this.configuration).listSpeechToTextModels(options).then((request) => request(this.axios, this.basePath));
    }

    /**
     * This endpoint allows you to send raw audio data in the request body for transcription. You can specify the desired model, language, output format, and various provider-specific features using query parameters. Suitable for transcribing local audio files. 
     * @summary Upload an audio file directly and receive a transcription.
     * @param {TranscriptionModelIdentifier} model The identifier of the speech-to-text model to use for the transcription, in the format &#x60;provider.model&#x60;. See the &#x60;/speech-to-text-models&#x60; endpoint for available models.
     * @param {File} body The audio file to transcribe. Send the raw audio data as the request body. Supported formats typically include WAV, MP3, FLAC, Ogg, M4A, etc., depending on the chosen model/provider. Check provider documentation for specific limits on file size and duration.
     * @param {TranscriptLanguageCode} [language] The language of the audio file in ISO 639-1 format (e.g., &#x60;en&#x60;, &#x60;es&#x60;, &#x60;fr&#x60;). Specify &#x60;auto&#x60; for automatic language detection (if supported by the model). Defaults to &#x60;en&#x60; if not provided. Providing the correct language improves accuracy and latency.
     * @param {TranscriptOutputFormat} [outputFormat] The desired format for the transcription output. Can be plain text, JSON objects (simple or detailed), or subtitle formats (SRT, VTT). Defaults to &#x60;text&#x60;.
     * @param {string} [rulesetId] The unique identifier (UUID) of a pre-defined replacement ruleset to apply to the final transcription text. Create rulesets using the &#x60;/replacement-rulesets&#x60; endpoint.
     * @param {boolean} [punctuation] Enable automatic punctuation (commas, periods, question marks) in the transcription. Support varies by model/provider (e.g., Deepgram, AssemblyAI). Defaults to &#x60;true&#x60;.
     * @param {TranscribeTimestampGranularityEnum} [timestampGranularity] Specifies the level of detail for timestamps in the response (if &#x60;output_format&#x60; is &#x60;json&#x60; or &#x60;verbose_json&#x60;). &#x60;segment&#x60; provides timestamps for larger chunks of speech, while &#x60;word&#x60; provides timestamps for individual words (may increase latency). Defaults to &#x60;segment&#x60;.
     * @param {boolean} [diarization] Enable speaker diarization to identify and label different speakers in the audio. Support and quality vary by model/provider. Defaults to &#x60;false&#x60;. When enabled, the &#x60;speaker&#x60; field may be populated in the response segments.
     * @param {string} [initialPrompt] An optional text prompt to provide context, guide the model\&#39;s style (e.g., spelling of specific names), or improve accuracy for subsequent audio segments. Support varies by model (e.g., OpenAI models).
     * @param {number} [temperature] Controls the randomness of the output for certain models (e.g., OpenAI). A value between 0 and 1. Lower values (e.g., 0.2) make the output more deterministic, while higher values (e.g., 0.8) make it more random. Defaults vary by model.
     * @param {boolean} [smartFormat] Enable provider-specific \&quot;smart formatting\&quot; features, which might include formatting for numbers, dates, currency, etc. Currently supported by Deepgram models. Defaults vary.
     * @param {number} [speakersExpected] Provides a hint to the diarization process about the number of expected speakers. May improve accuracy for some providers (e.g., RevAI, Deepgram).
     * @param {Array<string>} [customVocabulary] Provide a list of specific words or phrases (e.g., proper nouns, jargon) to increase their recognition likelihood. Support varies by provider (e.g., Deepgram, AssemblyAI).
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof SpeechToTextApi
     */
    public transcribe(model: TranscriptionModelIdentifier, body: File, language?: TranscriptLanguageCode, outputFormat?: TranscriptOutputFormat, rulesetId?: string, punctuation?: boolean, timestampGranularity?: TranscribeTimestampGranularityEnum, diarization?: boolean, initialPrompt?: string, temperature?: number, smartFormat?: boolean, speakersExpected?: number, customVocabulary?: Array<string>, options?: RawAxiosRequestConfig) {
        return SpeechToTextApiFp(this.configuration).transcribe(model, body, language, outputFormat, rulesetId, punctuation, timestampGranularity, diarization, initialPrompt, temperature, smartFormat, speakersExpected, customVocabulary, options).then((request) => request(this.axios, this.basePath));
    }

    /**
     * This endpoint allows you to transcribe an audio file hosted at a publicly accessible URL. Provide the URL and transcription options within the JSON request body. Useful for transcribing files already stored online. 
     * @summary Transcribe an audio file located at a remote URL.
     * @param {RemoteTranscriptionConfiguration} remoteTranscriptionConfiguration JSON object containing the URL of the audio file and the desired transcription options.
     * @param {*} [options] Override http request option.
     * @throws {RequiredError}
     * @memberof SpeechToTextApi
     */
    public transcribeRemote(remoteTranscriptionConfiguration: RemoteTranscriptionConfiguration, options?: RawAxiosRequestConfig) {
        return SpeechToTextApiFp(this.configuration).transcribeRemote(remoteTranscriptionConfiguration, options).then((request) => request(this.axios, this.basePath));
    }
}

/**
 * @export
 */
export const TranscribeTimestampGranularityEnum = {
    Word: 'word',
    Segment: 'segment'
} as const;
export type TranscribeTimestampGranularityEnum = typeof TranscribeTimestampGranularityEnum[keyof typeof TranscribeTimestampGranularityEnum];


